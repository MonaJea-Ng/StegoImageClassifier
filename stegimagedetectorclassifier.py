# -*- coding: utf-8 -*-
"""StegImageDetectorClassifier

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EVvKArCUVUZoS7Owo-DH-NPOC5_fjxoi

Install Tensorflow


!pip install --upgrade tensorflow
!pip install --upgrade keras

!pip install kaggle

!pip install optuna
!pip install optuna-integration[tfkeras]

Custom Entropy-Based Feature Extraction Layer"""

'''
[/] The formula for this layer will be based on Shannonâ€™s entropy. where P(x)
is the probability of the outcome, with the logarithm usually taken base 2, and the
unit of entropy is bits. The researchers will use this formula in detecting
anomalies in the image.

    In getting the spatial entropy:
    [/] the pixel values will be flattened.
    [/] sliding window technique will be used in order to compute the entropy of each RGB value.
    [] the histogram of pixel values will be computed based on patch
    [/] normalized results by dividing each bin count by the total number of counts to get the probability distribution
    [] calculate the spatial entropy with the probability distribution.

    In getting the frequency entropy:
    [/] spatial domain data will be converted to frequency domain data using Domain Cosine Transform (DCT)
    [/] DCT coefficients will be flattened
    [/] sliding window technique will be used in order to compute the entropy of each RGB value
    [] histogram of DCT Coefficients based on patch will be computed
    [/] normalized results by dividing each bin count by the total number of counts to get the probability distribution
    [/] Then, the frequency entropy will be calculated with the probability distribution.

    [] The result from spatial entropy and frequency entropy will be calculated using joint entropy
    [] return the result to the model.

Other Tasks:
[] Debugging
[] Double Check formulas
[] Histogram / Histogram VIsualization
'''

"""Inception V3 Model"""

'''
Input: 512x512 PNG image

Model

[/] Detection - for the purpose of knowing whether the image has malicious payloads or not
  [/] Stem block - contains the ff (conv 3x3 -> conv 3x3 -> conv 3x3 -> max pooling layer -> conv 3x3 -> conv 3x3 -> max pooling layer)
  [/] Inception Module 1 - 3 sets of convolutional layers
  [/] Entropy-based feature extraction layer - information on specific code block
  [/] Inception Module 2 - 3 sets of convolutional layers ()
  [/] Auxilliary Classifier
  [/] Inception Module 3 - 3 sets of convolutional layers ()

[/] Classification
  [/] Global Average Pooling layer
  [/] Dense Layer 1
  [/] ReLU
  [/] Dropout
  [/] Dense Layer 2
  [/] Softmax

# Output: Clean, Malicious (URL, PS, JS in HTML, JS, Ethereum Address)

[] Debugging :<<
[] Model Training
[] Export
[/] UI

Optional:
[] Histogram Visualization from results in probability distribution
[] 2 options for detection only vs to classify (includes detection) instead of merely scanning

'''

import tensorflow as tf
import numpy as np
import os
import shutil
import pickle
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Concatenate, GlobalAveragePooling2D, Dense, ReLU, Dropout, Softmax, Flatten, Input, AveragePooling2D, RepeatVector, Reshape, Lambda, Layer
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import register_keras_serializable
from scipy.fftpack import dct
from keras import backend as K

@register_keras_serializable()
class EntropyBasedFeatureExtractionLayer(tf.keras.layers.Layer):

    def __init__(self, patch_size=(16, 16, 224), stride=8, num_bins=256, **kwargs):
        super(EntropyBasedFeatureExtractionLayer, self).__init__(**kwargs)
        self.patch_size = patch_size
        self.stride = stride
        self.num_bins = num_bins

    def get_config(self):
        config = super(EntropyBasedFeatureExtractionLayer, self).get_config()
        return config

    def extract_patches(self, images):
        patches = tf.image.extract_patches(
            images=images,
            sizes=[1, self.patch_size[0], self.patch_size[1], 1],
            strides=[1, self.stride, self.stride, 1],
            rates=[1, 1, 1, 1],
            padding='VALID'
        )
        batch_size = tf.shape(images)[0]
        num_patches_h = tf.shape(patches)[1]
        num_patches_w = tf.shape(patches)[2]
        num_patches = num_patches_h * num_patches_w

        patches = tf.reshape(patches, (batch_size, num_patches, self.patch_size[0], self.patch_size[1], images.shape[-1]))
        return patches

    def compute_entropy(self, prob_dist):
        return -tf.reduce_sum(prob_dist * tf.math.log(prob_dist + 1e-10))

    def compute_spatial_entropy(self, patches):
        def compute_entropy_for_patch(patch):
            patch_entropy = []
            for channel in range(3):  # Assuming RGB input
                channel_values = patch[:, :, channel]
                hist = tf.histogram_fixed_width(channel_values, value_range=(0, 255), nbins=self.num_bins)
                hist = tf.cast(hist, tf.float32)
                hist /= tf.reduce_sum(hist + 1e-10)
                entropy = self.compute_entropy(hist)
                patch_entropy.append(entropy)
            return tf.reduce_mean(patch_entropy)

        entropies = tf.map_fn(compute_entropy_for_patch, patches)
        return entropies

    def compute_frequency_entropy(self, patches):
        def compute_entropy_for_patch(patch):
            patch_entropy = []
            for channel in range(3):
                channel_values = patch[:, :, channel]
                dct_patch = tf.signal.dct(tf.signal.dct(channel_values, norm='ortho'), norm='ortho')
                dct_values = tf.abs(tf.reshape(dct_patch, [-1]))

                # Check if tf.reduce_max(dct_values) is 0 and adjust value_range accordingly
                max_dct_value = tf.reduce_max(dct_values)
                value_range = tf.cond(
                    tf.equal(max_dct_value, 0),
                    lambda: (0, 1),  # Use a default range if max_dct_value is 0
                    lambda: (0, max_dct_value)
                )

                hist = tf.histogram_fixed_width(dct_values, value_range=value_range, nbins=self.num_bins)
                hist = tf.cast(hist, tf.float32)
                hist /= tf.reduce_sum(hist + 1e-10)
                entropy = self.compute_entropy(hist)
                patch_entropy.append(entropy)
            return tf.reduce_mean(patch_entropy)

        entropies = tf.map_fn(compute_entropy_for_patch, patches)
        return entropies

    def compute_joint_entropy(self, spatial_entropies, frequency_entropies):
        joint_entropies = spatial_entropies + frequency_entropies
        return joint_entropies

    def compute_attention_weights(self, joint_entropies):
        attention_weights = tf.nn.softmax(joint_entropies)
        return attention_weights

    def compute_output_shape(self, input_shape):
        """Computes the output shape of the layer.

        Args:
            input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer).
                         Shape tuples can include None for free dimensions, instead of an integer.

        Returns:
            Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer).
        """
        # Calculate the output shape based on the input shape and layer parameters
        batch_size = input_shape[0]
        # Assuming input_shape is (batch_size, height, width, channels)
        # Calculate the number of patches in height and width dimensions based on stride and patch_size
        num_patches_h = tf.math.floordiv(input_shape[1] - self.patch_size[0], self.stride) + 1
        num_patches_w = tf.math.floordiv(input_shape[2] - self.patch_size[1], self.stride) + 1
        # Output shape is (batch_size, top_n, patch_height, patch_width, channels)
        output_shape = (batch_size, self.patch_size[0], self.patch_size[1], input_shape[3])

        return output_shape

    def call(self, inputs):
        image = tf.convert_to_tensor(inputs)
        patches = self.extract_patches(image)

        # Compute spatial and frequency entropy for each patch
        spatial_entropies = self.compute_spatial_entropy(patches)
        frequency_entropies = self.compute_frequency_entropy(patches)

        # Compute joint entropy
        joint_entropies = self.compute_joint_entropy(spatial_entropies, frequency_entropies)

        # Compute attention weights based on joint entropies
        attention_weights = self.compute_attention_weights(joint_entropies)

        # Reshape attention_weights for broadcasting
        # Ensure attention_weights have at least 2 dimensions for reshape
        attention_weights = tf.expand_dims(attention_weights, axis=-1)  # Add a dimension at the end
        attention_weights = tf.reshape(attention_weights, (tf.shape(attention_weights)[0], tf.shape(attention_weights)[1], 1, 1, 1))

        # Gather patches based on attention weights (using weighted summation)
        weighted_patches = patches * attention_weights

        # Emphasize more suspicious areas by summing the weighted patches instead of averaging
        emphasized_features = tf.reduce_sum(weighted_patches, axis=1)  # Sum across patches

        # Return the emphasized features, which will have shape (batch_size, patch_height, patch_width, channels)
        return emphasized_features

## Detection - for the purpose of knowing whether the image has malicious payloads or not
@register_keras_serializable()
class StemBlock(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(StemBlock, self).__init__(**kwargs)

    def get_config(self):
        config = super(StemBlock, self).get_config()
        return config

    def call(self, x):
        x = Conv2D(32, (3, 3), activation='relu')(x)
        x = Conv2D(32, (3, 3), activation='relu')(x)
        x = Conv2D(64, (3, 3), activation='relu')(x)
        x = MaxPooling2D((3, 3))(x)
        x = Conv2D(80, (3, 3), activation='relu')(x)
        x = Conv2D(192, (3, 3), activation='relu')(x)
        x = MaxPooling2D((3, 3))(x)
        return x

# Inception Module 1
@register_keras_serializable()
class InceptionModule1(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(InceptionModule1, self).__init__(**kwargs)

    def get_config(self):
        config = super(InceptionModule1, self).get_config()
        return config

    def call(self, x):
        # branch 1: 1x1 Conv followed by 3x3 and 3x3 Convolution
        branch1x1 = Conv2D(64, (1, 1), padding='same', activation='relu')(x)
        branch1x1 = Conv2D(64, (3, 3), padding='same', activation='relu')(branch1x1)
        branch1x1 = Conv2D(64, (3, 3), padding='same', activation='relu')(branch1x1)

        # Branch 2: 1x1 Conv followed by 3x3 Convolution
        branch2_1x1 = Conv2D(64, (1, 1), padding='same', activation='relu')(x)
        branch2_3x3 = Conv2D(96, (3, 3), padding='same', activation='relu')(branch2_1x1)

        # Branch 3: Pooling followed by 1x1 Conv
        branch_pool = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)
        branch_pool = Conv2D(32, (1, 1), padding='same', activation='relu')(branch_pool)

        # Branch 4: 1x1 Conv only
        branch_1x1 = Conv2D(32, (1, 1), padding='same', activation='relu')(x)

        # Concatenate all branches
        output = Concatenate()([branch1x1, branch2_3x3, branch_pool, branch_1x1])

        return output

#INnception module 2
@register_keras_serializable()
class InceptionModule2(tf.keras.layers.Layer):

    def __init__(self, n=5, **kwargs):
        super(InceptionModule2, self).__init__(**kwargs)
        self.n = n

    def get_config(self):
        config = super(InceptionModule2, self).get_config()
        return config

    def call(self, x):

        # Branch 1: 1x1 Conv -> 1xn Conv -> nx1 Conv -> 1xn Conv -> nx1 Conv
        branch1 = Conv2D(64, (1, 1), padding='same', activation='relu')(x)
        branch1 = Conv2D(128, (1, self.n), padding='same', activation='relu')(branch1)  # Use self.n here
        branch1 = Conv2D(128, (self.n, 1), padding='same', activation='relu')(branch1)  # Use self.n here
        branch1 = Conv2D(128, (1, self.n), padding='same', activation='relu')(branch1)  # Use self.n here
        branch1 = Conv2D(128, (self.n, 1), padding='same', activation='relu')(branch1)  # Use self.n here

        # Branch 2: 1x1 Conv -> 1xn Conv -> nx1 Conv
        branch2 = Conv2D(64, (1, 1), padding='same', activation='relu')(x)
        branch2 = Conv2D(128, (1, self.n), padding='same', activation='relu')(branch2)  # Use self.n here
        branch2 = Conv2D(128, (self.n, 1), padding='same', activation='relu')(branch2)  # Use self.n here

        # Branch 3: Pooling followed by 1x1 Conv
        branch3 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)
        branch3 = Conv2D(64, (1, 1), padding='same', activation='relu')(branch3)

        # Branch 4: 1x1 Conv only
        branch4 = Conv2D(64, (1, 1), padding='same', activation='relu')(x)

        # Concatenate all branches
        output = Concatenate()([branch1, branch2, branch3, branch4])

        # Return both the main output and the auxiliary output
        return output

# Inception Module 3
@register_keras_serializable()
class InceptionModule3(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(InceptionModule3, self).__init__(**kwargs)

    def get_config(self):
        config = super(InceptionModule3, self).get_config()
        return config

    def call(self, x):
        # Branch 1
        branch1x1 = Conv2D(64, (1, 1), padding='same', activation='relu')(x)
        branch1x3 = Conv2D(128, (3, 3), padding='same', activation='relu')(branch1x1)
        branch1x3_1x3 = Conv2D(128, (1, 3), padding='same', activation='relu')(branch1x3)
        branch1x3_3x1 = Conv2D(128, (3, 1), padding='same', activation='relu')(branch1x3)

        # Branch 2
        branch2x1 = Conv2D(128, (1, 1), padding='same', activation='relu')(x)
        branch2x3_1x3 = Conv2D(128, (1, 3), padding='same', activation='relu')(branch2x1)
        branch2x1_3 = Conv2D(128, (3, 1), padding='same', activation='relu')(branch2x1)

        # Branch 3
        branch3_pool = AveragePooling2D(pool_size=(3, 3), strides=1, padding='same')(x)
        branch3x1 = Conv2D(128, (1, 1), padding='same', activation='relu')(branch3_pool)

        # Branch 4
        branch4x1 = Conv2D(128, (1, 1), padding='same', activation='relu')(x)

        # Concatenate all branches
        outputs = Concatenate(axis=-1)([branch1x3_1x3, branch1x3_3x1, branch2x3_1x3, branch2x1_3, branch3x1, branch4x1])

        return outputs

# Auxiliary Classifier

@register_keras_serializable()
class AuxiliaryClassifier(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(AuxiliaryClassifier, self).__init__(**kwargs)

    def get_config(self):
        config = super(AuxiliaryClassifier, self).get_config()
        return config

    def call(self, x):
        # Average Pooling layer (5x5 kernel, stride 3)
        # Added padding='same' to ensure output dimensions are compatible with subsequent layers.
        x = AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='same')(x)
        # Alternatively, reduce kernel size or stride to avoid negative dimensions
        #x = AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(x)

        # Conv2D layer (1x1 kernel, 128 filters, ReLU activation)
        x = Conv2D(128, (1, 1), padding='same', activation='relu')(x)
        x = Conv2D(128, (1, 1), padding='same', activation='relu')(x)

        # Flatten the output
        x = Flatten()(x)

        # Fully connected layer (1024 units, ReLU activation)
        x = Dense(246, activation='relu')(x)

        # Dropout (0.7 keep probability)
        x = Dropout(0.7)(x)

        return x
    

def build_model(input_shape, num_classes):
    
    input_img = tf.keras.Input(shape=input_shape, batch_size=32)

    # Use class instances
    x = StemBlock()(input_img)
    x = InceptionModule1()(x)

    entropy_layer = EntropyBasedFeatureExtractionLayer()
    x = entropy_layer(x)

    x = InceptionModule2()(x)

    aux_dropout = AuxiliaryClassifier()(x)

    auxiliary_output = Dense(num_classes, activation='softmax', name='auxiliary_output')(aux_dropout)

    x = InceptionModule3()(x)
    # Global Average Pooling
    gap = GlobalAveragePooling2D()(x)

    concatenated = Concatenate()([auxiliary_output, gap])

    # Dense Layers for Classification
    x = Dense(1024, activation='relu')(concatenated)
    x = Dropout(0.5)(x)
    # Get the main output
    main_output = Dense(num_classes, activation='softmax', name='main_output')(x)

    # Model Definition
    model = Model(inputs=input_img, outputs=[auxiliary_output, main_output], name='stegoimageclassifier')


    return model


# Define the weights for the loss functions
auxiliary_output_weight = 1.0  # Weight for the auxiliary output
main_output_weight = 1.5  # Weight for the main output
num_classes = 6 

model = build_model((512, 512, 3), 6)  # Assuming 6 classes for classification

# Compile the model
model.compile(optimizer='adam',
              loss={
                   'auxiliary_output': 'categorical_crossentropy',  # Auxiliary output loss
                   'main_output': 'categorical_crossentropy'  # Main output loss
              },
              loss_weights={
                  'auxiliary_output': auxiliary_output_weight,  # Weight for auxiliary output
                  'main_output': main_output_weight          # Weight for main output
              },
              metrics={
                  'auxiliary_output': ['accuracy'],
                  'main_output': ['accuracy']})  # You can also add additional metrics here

# Model summary
model.summary()

''''
!kaggle datasets download -d marcozuppelli/stegoimagesdataset
!unzip -q stegoimagesdataset.zip -d ./stegoimagesdataset


# Define the categories based on filename substrings
categories = {
    'url': 'malicious_url',
    'js': 'malicious_js',
    'html': 'malicious_html',
    'ps': 'malicious_ps',
    'eth': 'malicious_ethereum'
}

# Paths to train, test, and val directories
train_dir = 'stegoimagesdataset/train/train'
test_dir = 'stegoimagesdataset/test/test'
val_dir = 'stegoimagesdataset/val/val'

# Paths to destination directories
train_dest = 'train'
test_dest = 'test'
val_dest = 'val'

def classify_images(source_dir, destination_dir):
    # Create directories for each category
    for category in categories.values():
        os.makedirs(os.path.join(destination_dir, category), exist_ok=True)
    os.makedirs(os.path.join(destination_dir, 'clean'), exist_ok=True)

    # Classify images based on filenames
    for subfolder in os.listdir(source_dir):
        subfolder_path = os.path.join(source_dir, subfolder)

        if os.path.isdir(subfolder_path):
            for filename in os.listdir(subfolder_path):
                filepath = os.path.join(subfolder_path, filename)

                # Check if the filename contains any of the keywords and move accordingly
                moved = False
                for keyword, category in categories.items():
                    if keyword in filename.lower():  # Convert to lowercase for case-insensitive matching
                        dest_path = os.path.join(destination_dir, category, filename)
                        shutil.move(filepath, dest_path)
                        moved = True
                        print(f"Moved {filename} to {category}")
                        break

                # If no keyword is found, it's clean
                if not moved:
                    dest_path = os.path.join(destination_dir, 'clean', filename)
                    shutil.move(filepath, dest_path)
                    print(f"Moved {filename} to clean")

# Classify images in each dataset
classify_images(train_dir, train_dest)
classify_images(test_dir, test_dest)
classify_images(val_dir, val_dest)

print("Classification complete!")

# Clear previous session
tf.keras.backend.clear_session()

tf.config.run_functions_eagerly(True)

# Limit GPU memory growth
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# Enable mixed precision (if desired)
# tf.keras.mixed_precision.set_global_policy('mixed_float16')

# Set up data generators with reduced image size
train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1.0 / 255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest',
    validation_split=0.2
)

train_generator = train_datagen.flow_from_directory(
    'train',
    target_size=(512, 512),  # Reduced size
    batch_size=8,  # Further reduced batch size
    class_mode='categorical',
    subset='training'
)

validation_generator = train_datagen.flow_from_directory(
    'val',
    target_size=(512, 512),  # Reduced size
    batch_size=8,  # Further reduced batch size
    class_mode='categorical',
    subset='validation'
)


# Callbacks for early stopping, model checkpointing, and custom history saving
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=5,
    min_delta=0.001,
    restore_best_weights=True
)


checkpoint = ModelCheckpoint(
    'model_checkpoint.keras',
    monitor='val_loss',
    save_best_only=False,  # Save after every epoch
    save_weights_only=False,
    verbose=1
)

class SaveHistory(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        with open('training_history.pkl', 'wb') as f:
            pickle.dump(self.model.history.history, f)


# Compile callbacks into a list (removed early stopping at index 0)
callbacks = [checkpoint, SaveHistory()]

# Define the weights for the loss functions
auxiliary_output_weight = 1.0   # Weight for the auxiliary output
main_output_weight = 1.5  # Weight for the main output

# Check if the model checkpoint file exists, or create a new model if not.
if os.path.exists('model_checkpoint.keras') and os.path.getsize('model_checkpoint.keras') > 0:
    print("Loading existing model from checkpoint.")
    # Load the saved model if it exists
    model = load_model('model_checkpoint.keras', custom_objects={
        'StemBlock': StemBlock,
        'InceptionModule1': InceptionModule1,
        'EntropyBasedFeatureExtractionLayer': EntropyBasedFeatureExtractionLayer,
        'InceptionModule2': InceptionModule2,
        'AuxiliaryClassifier': AuxiliaryClassifier,
        'InceptionModule3': InceptionModule3
    })
    # Load the training history if it exists
    if os.path.exists('training_history.pkl'):
        with open('training_history.pkl', 'rb') as f:
            history_data = pickle.load(f)
        # Optionally, you can inspect the training history
        print("Loaded training history.")
        print(history_data)
    else:
        print("Training history file not found. Starting from scratch.")
else:
    print("Model checkpoint not found. Creating a new model.")
    model = build_model((512, 512, 3), 6)


# Compile the model to include both outputs
model.compile(optimizer='adam',
              loss={
                  'auxiliary_output': 'categorical_crossentropy',
                  'main_output': 'categorical_crossentropy'  # Main output loss
              },
              loss_weights={
                  'auxiliary_output': auxiliary_output_weight,  # Weight for auxiliary output
                  'main_output': main_output_weight       # Weight for main output
              },
              metrics={
                  'auxiliary_output': ['accuracy'],
                  'main_output': ['accuracy']
              })

# Fit the model and make sure both outputs' metrics are visible
# Resume training from epoch 3 (or last completed epoch)
history = model.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=10,  # Total epochs
    initial_epoch=0,  # Change in case of crashes
    callbacks=callbacks,
    verbose=1
)

import tensorflow as tf
from tensorflow.keras.models import load_model
from google.colab import files  # Only needed in Google Colab

# Assuming 'model' is your trained model and 'val_data' and 'val_labels' are your validation data

# Step 1: Evaluate the Model
# train_loss, train_aux_loss, train_main_loss, train_aux_accuracy, train_main_accuracy = model.evaluate(train_generator)
# print(f'Train loss: {train_loss}, Auxiliary Loss: {train_aux_loss}, Main Loss: {train_main_loss}')
# print(f'Auxiliary Accuracy: {train_aux_accuracy}, Main Accuracy: {train_main_accuracy}')

# Step 2: Save the Model
model.save('stegoimageclassifier.h5')  # Save the model as an HDF5 file

model.save('stegoimageclassifier.keras')

# Step 3: Download the Model (only in Google Colab)
files.download('stegoimageclassifier.h5')
files.download('stegoimageclassifier.keras')

from google.colab import files

# Download the model checkpoint file
files.download('model_checkpoint.keras')
'''

"""

---

"""